{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN09ovr1C1p46/McpWnsP/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divy-arun-mav/handwriting-Recognition/blob/main/handwritingRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jW6eCQtwOsyH"
      },
      "outputs": [],
      "source": [
        "# collecting data\n",
        "!wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
        "!unzip -qq IAM_Words.zip\n",
        "!\n",
        "!mkdir data\n",
        "!mkdir data/words\n",
        "!tar -xf IAM_Words/words.tgz -C data/words\n",
        "!mv IAM_Words/words.txt data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 data/words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkBWz9kPRRL0",
        "outputId": "b06c5ebd-ed02-4909-bbe5-411bdaf46d15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#--- words.txt ---------------------------------------------------------------#\n",
            "#\n",
            "# iam database word information\n",
            "#\n",
            "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
            "#\n",
            "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
            "#     ok              -> result of word segmentation\n",
            "#                            ok: word was correctly\n",
            "#                            er: segmentation of word can be bad\n",
            "#\n",
            "#     154             -> graylevel to binarize the line containing this word\n",
            "#     1               -> number of components for this word\n",
            "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
            "#     AT              -> the grammatical tag for this word, see the\n",
            "#                        file tagset.txt for an explanation\n",
            "#     A               -> the transcription for this word\n",
            "#\n",
            "a01-000u-00-00 ok 154 408 768 27 51 AT A\n",
            "a01-000u-00-01 ok 154 507 766 213 48 NN MOVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "rZznJ_ZWRW8j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"data\"\n",
        "words_list = []\n",
        "\n",
        "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
        "for line in words:\n",
        "    if line[0] == \"#\":\n",
        "        continue\n",
        "    if line.split(\" \")[1] != \"err\":\n",
        "        words_list.append(line)\n",
        "\n",
        "len(words_list)\n",
        "np.random.shuffle(words_list)"
      ],
      "metadata": {
        "id": "INeR15qjSKF9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words_list[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzXVjzXKTP8i",
        "outputId": "5a4c4b98-10ac-4d76-a8bc-1a4d913ae305"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['e04-030-04-08 ok 170 1489 1499 120 39 JJ sure\\n', 'k02-102-05-03 ok 182 836 1623 69 52 PP3A he\\n', 'a01-082u-01-04 ok 172 1582 1043 234 88 IN during\\n', 'm01-000-07-00 ok 196 339 1998 75 107 INO of\\n', 'g01-031-07-06 ok 152 1912 2038 167 59 NN booty\\n', 'f07-081b-01-06 ok 168 1366 924 350 88 NN gastronomy\\n', 'n03-082-04-03 ok 165 992 1414 118 135 NN boy\\n', 'g06-018c-04-05 ok 182 1298 1438 96 58 ATI The\\n', 'g06-011j-06-06 ok 182 1222 1785 146 95 CC and\\n', 'f04-024-01-06 ok 183 1104 981 60 70 IN in\\n', 'g06-050k-00-07 ok 156 1842 717 85 76 PP$ his\\n', 'm01-100-01-06 ok 185 1459 880 177 131 JJ flying\\n', 'n02-033-04-01 ok 149 940 1486 353 85 VBD presented\\n', 'p03-047-00-01 ok 191 393 739 127 80 UH No\\n', 'g06-011e-08-00 ok 154 386 2190 70 44 CS as\\n', 'p02-000-00-02 ok 182 752 727 97 85 AP few\\n', 'c01-009-08-03 ok 182 1387 2363 170 81 NN work\\n', 'c04-156-01-04 ok 175 989 912 65 142 INO of\\n', 'h02-004-09-01 ok 191 776 2424 191 76 NNS items\\n', 'g06-037o-03-02 ok 188 850 1274 214 75 VBD took\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.9 * len(words_list))\n",
        "train_samples = words_list[:split_idx]\n",
        "test_samples = words_list[split_idx:]\n",
        "\n",
        "val_split_idx = int(0.5 * len(test_samples))\n",
        "validation_samples = test_samples[:val_split_idx]\n",
        "test_samples = test_samples[val_split_idx:]\n",
        "\n",
        "assert len(words_list) == len(train_samples) + len(validation_samples) + len(test_samples)\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "print(f\"Total validation samples: {len(validation_samples)}\")\n",
        "print(f\"Total test samples: {len(test_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLOZ8IVLTk4N",
        "outputId": "fe9f84e0-2d8c-4125-fde2-9d06b64eb729"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 86810\n",
            "Total validation samples: 4823\n",
            "Total test samples: 4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = os.path.join(base_path, \"words\")\n",
        "\n",
        "def get_image_paths_and_labels(samples):\n",
        "    paths = []\n",
        "    corrected_samples = []\n",
        "    for (i, file_line) in enumerate(samples):\n",
        "        line_split = file_line.strip()\n",
        "        line_split = line_split.split(\" \")\n",
        "\n",
        "        image_name = line_split[0]\n",
        "        partI = image_name.split(\"-\")[0]\n",
        "        partII = image_name.split(\"-\")[1]\n",
        "\n",
        "        img_path = os.path.join(\n",
        "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
        "        )\n",
        "\n",
        "        if os.path.getsize(img_path):\n",
        "            paths.append(img_path)\n",
        "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
        "\n",
        "    return paths, corrected_samples\n",
        "\n",
        "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
        "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
        "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
      ],
      "metadata": {
        "id": "lHG5mTw0X6xN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_paths[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz1K227ra0K2",
        "outputId": "61085d47-e00f-4d6e-83b1-2a73e2f56d9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/words/e04/e04-030/e04-030-04-08.png',\n",
              " 'data/words/k02/k02-102/k02-102-05-03.png',\n",
              " 'data/words/a01/a01-082u/a01-082u-01-04.png',\n",
              " 'data/words/m01/m01-000/m01-000-07-00.png',\n",
              " 'data/words/g01/g01-031/g01-031-07-06.png',\n",
              " 'data/words/f07/f07-081b/f07-081b-01-06.png',\n",
              " 'data/words/n03/n03-082/n03-082-04-03.png',\n",
              " 'data/words/g06/g06-018c/g06-018c-04-05.png',\n",
              " 'data/words/g06/g06-011j/g06-011j-06-06.png',\n",
              " 'data/words/f04/f04-024/f04-024-01-06.png',\n",
              " 'data/words/g06/g06-050k/g06-050k-00-07.png',\n",
              " 'data/words/m01/m01-100/m01-100-01-06.png',\n",
              " 'data/words/n02/n02-033/n02-033-04-01.png',\n",
              " 'data/words/p03/p03-047/p03-047-00-01.png',\n",
              " 'data/words/g06/g06-011e/g06-011e-08-00.png',\n",
              " 'data/words/p02/p02-000/p02-000-00-02.png',\n",
              " 'data/words/c01/c01-009/c01-009-08-03.png',\n",
              " 'data/words/c04/c04-156/c04-156-01-04.png',\n",
              " 'data/words/h02/h02-004/h02-004-09-01.png',\n",
              " 'data/words/g06/g06-037o/g06-037o-03-02.png']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy9azm2Ja_Qk",
        "outputId": "24cef6d9-bbe1-4ab1-a258-2319946f4e64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e04-030-04-08 ok 170 1489 1499 120 39 JJ sure',\n",
              " 'k02-102-05-03 ok 182 836 1623 69 52 PP3A he',\n",
              " 'a01-082u-01-04 ok 172 1582 1043 234 88 IN during',\n",
              " 'm01-000-07-00 ok 196 339 1998 75 107 INO of',\n",
              " 'g01-031-07-06 ok 152 1912 2038 167 59 NN booty',\n",
              " 'f07-081b-01-06 ok 168 1366 924 350 88 NN gastronomy',\n",
              " 'n03-082-04-03 ok 165 992 1414 118 135 NN boy',\n",
              " 'g06-018c-04-05 ok 182 1298 1438 96 58 ATI The',\n",
              " 'g06-011j-06-06 ok 182 1222 1785 146 95 CC and',\n",
              " 'f04-024-01-06 ok 183 1104 981 60 70 IN in',\n",
              " 'g06-050k-00-07 ok 156 1842 717 85 76 PP$ his',\n",
              " 'm01-100-01-06 ok 185 1459 880 177 131 JJ flying',\n",
              " 'n02-033-04-01 ok 149 940 1486 353 85 VBD presented',\n",
              " 'p03-047-00-01 ok 191 393 739 127 80 UH No',\n",
              " 'g06-011e-08-00 ok 154 386 2190 70 44 CS as',\n",
              " 'p02-000-00-02 ok 182 752 727 97 85 AP few',\n",
              " 'c01-009-08-03 ok 182 1387 2363 170 81 NN work',\n",
              " 'c04-156-01-04 ok 175 989 912 65 142 INO of',\n",
              " 'h02-004-09-01 ok 191 776 2424 191 76 NNS items',\n",
              " 'g06-037o-03-02 ok 188 850 1274 214 75 VBD took']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_cleaned = []\n",
        "characters = set()\n",
        "max_len = 0\n",
        "\n",
        "for label in train_labels:\n",
        "    label = label.split(\" \")[-1].strip()\n",
        "    for char in label:\n",
        "      characters.add(char)\n",
        "    max_len = max(max_len, len(label))\n",
        "    train_labels_cleaned.append(label)\n",
        "print(\"Maximum Length: \",max_len)\n",
        "print(\"Vocab Size: \",len(characters))\n",
        "train_labels_cleaned[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4FSITBvbMMd",
        "outputId": "c2e64293-607b-4de3-8f57-1c3fa60fc9be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Length:  21\n",
            "Vocab Size:  78\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sure',\n",
              " 'he',\n",
              " 'during',\n",
              " 'of',\n",
              " 'booty',\n",
              " 'gastronomy',\n",
              " 'boy',\n",
              " 'The',\n",
              " 'and',\n",
              " 'in',\n",
              " 'his',\n",
              " 'flying',\n",
              " 'presented',\n",
              " 'No',\n",
              " 'as',\n",
              " 'few',\n",
              " 'work',\n",
              " 'of',\n",
              " 'items',\n",
              " 'took']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_labels(labels):\n",
        "    cleaned_labels = []\n",
        "    for label in labels:\n",
        "        label = label.split(\" \")[-1].strip()\n",
        "        cleaned_labels.append(label)\n",
        "    return cleaned_labels\n",
        "\n",
        "validation_labels_cleaned = clean_labels(validation_labels)\n",
        "test_labels_cleaned = clean_labels(test_labels)"
      ],
      "metadata": {
        "id": "HrzHObEgdjdF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
        "num_to_char = StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)"
      ],
      "metadata": {
        "id": "D5qfExfcduTH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVf7HvzPepCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}